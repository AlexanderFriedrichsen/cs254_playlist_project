{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2s023VS6NvV5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import spotipy\n",
        "import sqlite3\n",
        "from sklearn.cluster import KMeans\n",
        "from sqlite3 import Error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from openTSNE import TSNE as openTSNE\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS as STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GWnx-v7ONvWA"
      },
      "outputs": [],
      "source": [
        "def connect_db(db_file):\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "    return conn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IwvA8npiNvWB"
      },
      "outputs": [],
      "source": [
        "# connect to database\n",
        "# conn = connect_db('data.db')\n",
        "# cur = conn.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the target data\n",
        "# Extract playlists that have the input words in their titles to use as training data\n",
        "# input_words = ['summer', 'beach', 'throwbacks']\n",
        "# sql_select = '''SELECT AVG(danceability), AVG(energy), AVG(key), AVG(loudness), \n",
        "# AVG(mode), AVG(speechiness), AVG(acousticness), AVG(instrumentalness), AVG(liveness), \n",
        "# AVG(valence), AVG(tempo), AVG(duration_ms), AVG(time_signature) FROM avg_features_by_playlist WHERE'''\n",
        "def create_train(input_words, cur):\n",
        "    sql_select = '''SELECT * FROM avg_features_by_playlist WHERE'''\n",
        "    for w in range(len(input_words)):\n",
        "        sql_select += \" name LIKE '%\" + input_words[w] + \"%'\"\n",
        "        if w != len(input_words) -1:\n",
        "            sql_select += \" OR\" \n",
        "    pl_train = cur.execute(sql_select).fetchall()\n",
        "    return pl_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PCfDJLzmNvWB",
        "outputId": "994a0280-a025-4982-83fd-bfe5a120c0c3"
      },
      "outputs": [],
      "source": [
        "def get_x_data(pl_train, cur):\n",
        "    # Get X data (average playlist features)\n",
        "    data_cols = ['pid', 'name', 'danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature']\n",
        "    feature_cols = ['danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature']\n",
        "    test_cols = ['pid', 'name', 'danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature']\n",
        "\n",
        "    data_rows = cur.execute('select * from avg_features_by_playlist where pid > (select MIN(pid) from avg_features_by_playlist) order by pid').fetchall()\n",
        "\n",
        "\n",
        "    # test_pl = cur.execute('''select tracks_in_playlist.pid, tracks_in_playlist.track_uri, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature \n",
        "    # from features_by_track left join tracks_in_playlist on\n",
        "    # features_by_track.track_uri = tracks_in_playlist.track_uri \n",
        "    # where pid = (select MIN(pid) from avg_features_by_playlist)''').fetchall()\n",
        "\n",
        "\n",
        "    test_data = pd.DataFrame(pl_train, columns=test_cols)\n",
        "    data = pd.DataFrame(data_rows, columns=data_cols)\n",
        "\n",
        "    return test_data, test_cols, data, data_cols, feature_cols\n",
        "    # print(test_data.head())\n",
        "    # print(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ghXip9j4NvWD",
        "outputId": "b190b543-e694-46f0-a331-07272e7562a0"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "\n",
        "def scale_data(test_data, data, feature_cols):\n",
        "    y = test_data[feature_cols].mean()\n",
        "\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True).fit(data[feature_cols].values)\n",
        "\n",
        "    scaled_x = scaler.transform(data[feature_cols].values)\n",
        "    scaled_y = scaler.transform(np.array(y).reshape(1,-1))\n",
        "    scaled_features = pd.DataFrame(scaled_x)\n",
        "    tsne = openTSNE(perplexity=30, metric='euclidean', n_jobs=-1, random_state=0, verbose=False)\n",
        "    tsne_transformer = tsne.fit(scaled_x)\n",
        "    data_df = pd.DataFrame(tsne_transformer.transform(scaled_x), columns =['X', 'Y'])\n",
        "\n",
        "    #print(data_df, scaled_x, scaled_y, scaled_features)\n",
        "    return data_df, scaled_x, scaled_y, scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qf9YiqzqNvWE",
        "outputId": "7cac83b7-e7e3-4125-a6ce-0f8ee47f6d64"
      },
      "outputs": [],
      "source": [
        "def draw_scatterplot(data_df):\n",
        "    sns.scatterplot(x='X', y='Y', data=data_df, legend=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MxtQj6Khbe4e"
      },
      "outputs": [],
      "source": [
        "def calculate_num_clusters(scaled_x):\n",
        "    #TODO:\n",
        "    # needs to be tested and K updated below\n",
        "    #calculate how many K clusters there should be \n",
        "    wcss = [] \n",
        "    for number_of_clusters in range(1, 30): \n",
        "        kmeans = KMeans(n_clusters = number_of_clusters, random_state = 42)\n",
        "        kmeans.fit(scaled_x) \n",
        "        wcss.append(kmeans.inertia_)\n",
        "    wcss\n",
        "\n",
        "    ks = range(1, 30)\n",
        "    plt.plot(ks, wcss)\n",
        "    plt.axvline(4, linestyle='--', color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HboT_UKqNvWF"
      },
      "outputs": [],
      "source": [
        "def kmeans_init(data_df, scaled_x, scaled_y, n_clusters):\n",
        "    # initialize KMeans\n",
        "    \n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    clusters = kmeans.fit(scaled_x)\n",
        "    labels = clusters.labels_\n",
        "    data_df['cluster'] = pd.Categorical(labels)\n",
        "    \n",
        "    target_cluster = kmeans.predict(scaled_y)\n",
        "    return (target_cluster, labels, data_df['cluster'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "czsdHYnjNvWG",
        "outputId": "437142d0-73d2-4d1b-925b-0643ca9bddaa"
      },
      "outputs": [],
      "source": [
        "def draw_colored_scatterplot(data_df):\n",
        "    sns.scatterplot(x='X', y='Y', hue='cluster', style='cluster', data=data_df, legend=None)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wordclouds(data_df, data, n_clusters):\n",
        "    data_df[\"playlist_name\"] = data['name'].str.lower()\n",
        "    # display(data_df)\n",
        "\n",
        "    # we want to perform a pivot on data_df so that each cluster number is a column with row value equal to the playlist name.\n",
        "    # from there we can sum up that column to get the whole lsit of strings of playlist names for each cluster (column)\n",
        "    original_df = data_df.pivot(index='X', columns='cluster')['playlist_name'].reset_index()\n",
        "    original_df.columns.name = None\n",
        "    original_df = original_df.fillna('')\n",
        "    # original_df.head(20)\n",
        "    # print(original_df.columns)\n",
        "\n",
        "    #list of words to ignore\n",
        "    stop_words = STOPWORDS.update([\"i\", \"it\", \"me\", \"my\", \"that\", \"the\", \"of\", \"than\", \"then\", \n",
        "    \"when\", \"if\", \"a\", \"there\", \"playlist\", \"music\", \"song\", \"songs\", \"to\", \"too\", \"get\", \"as\", \"this\", \n",
        "    \"am\", \"is\", \"are\", \"has\", \"and\", \"aa\", \"aaa\", 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', \n",
        "    'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
        "\n",
        "    wordclouds = {}\n",
        "    for i in range(n_clusters):\n",
        "        original_df.replace(np.nan, '')\n",
        "        original_df[i] = original_df[i].astype(str) #.sum(skipna=True))\n",
        "        wordclouds[\"wordcloud\" + str(i)] = WordCloud(stopwords=stop_words).generate(' '.join(original_df[i]))\n",
        "\n",
        "    for val in wordclouds.values():\n",
        "        plt.imshow(val)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_target_cluster_songs(target_cluster, data_df, data):\n",
        "    data_df[\"pid\"] = data['pid']\n",
        "    original_df = data_df.pivot(columns='cluster', values='pid')\n",
        "    target_cluster_df = original_df[[target_cluster]].dropna()\n",
        "    conn = connect_db('data.db')\n",
        "    cur = conn.cursor()\n",
        "    \n",
        "    print(target_cluster_df)\n",
        "    exit()\n",
        "    num_of_ors = 0\n",
        "    \n",
        "    sql = '''SELECT features_by_track.* FROM features_by_track join tracks_in_playlist on features_by_track.track_uri = tracks_in_playlist.track_uri\n",
        "        WHERE '''\n",
        "    \n",
        "    result = []\n",
        "    \n",
        "    for pid in target_cluster_df[target_cluster].values:\n",
        "        sql += \"tracks_in_playlist.pid = \" + str(int(pid))\n",
        "        sql += \" OR \"\n",
        "        num_of_ors += 1\n",
        "        if (num_of_ors == 100):\n",
        "            sql = sql[:-3]\n",
        "            cols = [\"track_uri\",\n",
        "                \"danceability\",\n",
        "                \"energy\",\n",
        "                \"key\",\n",
        "                \"loudness\",\n",
        "                \"mode\",\n",
        "                \"speechiness\",\n",
        "                \"acousticness\",\n",
        "                \"instrumentalness\",\n",
        "                \"liveness\",\n",
        "                \"valence\",\n",
        "                \"tempo\",\n",
        "                \"duration_ms\",\n",
        "                \"time_signature\"]\n",
        "            result.append(cur.execute(sql).fetchall())\n",
        "            sql = '''SELECT features_by_track.* FROM features_by_track join tracks_in_playlist on features_by_track.track_uri = tracks_in_playlist.track_uri\n",
        "            WHERE '''\n",
        "            num_of_ors = 0\n",
        "        \n",
        "    conn.close()\n",
        "    tracks = pd.DataFrame(result, columns = cols)\n",
        "    tracks = tracks[tracks['track_uri'].map(tracks['track_uri'].value_counts()) > 3]\n",
        "    \n",
        "    tracks['counts'] = tracks.groupby(['track_uri'])['time_signature'].transform('count')\n",
        "    \n",
        "    tracks = tracks.drop_duplicates(subset=['track_uri'])\n",
        "    song_instances = tracks['counts'].sum()\n",
        "\n",
        "    \n",
        "    #num_unique_songs = len(pd.unique(tracks['track_uri']))\n",
        "    print(song_instances, \" song instances fetched\")\n",
        "    print(len(tracks), \" unique songs fetched\")\n",
        "    print(tracks.head(5))\n",
        "    return tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "generate_playlist\n",
        "\n",
        "Parameters:\n",
        "input_words: (String) to pass to get_target_cluster_songs\n",
        "obscurity: (float) int\n",
        "max_song_length: (int) max_length of any song in the playlist\n",
        "use_minutes: (bool) use minutes instead of number of songs for playlist length (default false)\n",
        "playlist_length: number of songs in the playlist. If use_minutes=true, minutes in the playlist\n",
        "'''\n",
        "\n",
        "def generate_playlist(tracks, obscurity=1, max_song_length=10, use_minutes=False, playlist_length=10):\n",
        "    playlist = []\n",
        "    \n",
        "    \n",
        "    # obscurity\n",
        "    # length\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline\n",
        "def pipeline():\n",
        "    # connect to database\n",
        "    conn = connect_db('data.db')\n",
        "    cur = conn.cursor()\n",
        "    input_words = [\"beach\",\"sun\"]\n",
        "    pl_train = create_train(input_words, cur)\n",
        "    test_data, test_cols, data, data_cols, feature_cols= get_x_data(pl_train, cur)\n",
        "    conn.close()\n",
        "    data_df, scaled_x, scaled_y, scaled_features = scale_data(test_data, data, feature_cols)\n",
        "    #draw_scatterplot(data_df) \n",
        "    #calculate_num_clusters(scaled_x) # unfinished\n",
        "    n_clusters = 20 # manually set after looking at calculate_num_clusters\n",
        "    target_cluster = kmeans_init(data_df, scaled_x, scaled_y, n_clusters)\n",
        "    #wordclouds(data_df, data, n_clusters)\n",
        "    # tracks = get_target_cluster_songs(target_cluster, data_df, data)\n",
        "\n",
        "    #generate_playlist(tracks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# connect to database\n",
        "conn = connect_db('data.db')\n",
        "cur = conn.cursor()\n",
        "input_words = [\"beach\",\"sun\"]\n",
        "pl_train = create_train(input_words, cur)\n",
        "test_data, test_cols, data, data_cols, feature_cols= get_x_data(pl_train, cur)\n",
        "conn.close()\n",
        "data_df, scaled_x, scaled_y, scaled_features = scale_data(test_data, data, feature_cols)\n",
        "#draw_scatterplot(data_df) \n",
        "#calculate_num_clusters(scaled_x) # unfinished\n",
        "n_clusters = 20 # manually set after looking at calculate_num_clusters\n",
        "target_cluster, labels, clusters = kmeans_init(data_df, scaled_x, scaled_y, n_clusters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cluster        19\n",
            "8             9.0\n",
            "32           33.0\n",
            "53           54.0\n",
            "62           63.0\n",
            "82           83.0\n",
            "...           ...\n",
            "999940   999941.0\n",
            "999944   999945.0\n",
            "999966   999967.0\n",
            "999977   999978.0\n",
            "999979   999980.0\n",
            "\n",
            "[71109 rows x 1 columns]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m target_cluster_songs \u001b[39m=\u001b[39m get_target_cluster_songs(target_cluster[\u001b[39m0\u001b[39;49m], data_df, data)\n",
            "Cell \u001b[1;32mIn [43], line 37\u001b[0m, in \u001b[0;36mget_target_cluster_songs\u001b[1;34m(target_cluster, data_df, data)\u001b[0m\n\u001b[0;32m     22\u001b[0m sql \u001b[39m=\u001b[39m sql[:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\n\u001b[0;32m     23\u001b[0m cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mtrack_uri\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdanceability\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39menergy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mduration_ms\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtime_signature\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 37\u001b[0m result\u001b[39m.\u001b[39mappend(cur\u001b[39m.\u001b[39;49mexecute(sql)\u001b[39m.\u001b[39;49mfetchall())\n\u001b[0;32m     38\u001b[0m sql \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\u001b[39mSELECT features_by_track.* FROM features_by_track join tracks_in_playlist on features_by_track.track_uri = tracks_in_playlist.track_uri\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[39mWHERE \u001b[39m\u001b[39m'''\u001b[39m\n\u001b[0;32m     40\u001b[0m num_of_ors \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "target_cluster_songs = get_target_cluster_songs(target_cluster[0], data_df, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[19]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Could not interpret value `X` for parameter `x`",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(target_cluster)\n\u001b[1;32m----> 2\u001b[0m draw_colored_scatterplot(target_cluster)\n",
            "Cell \u001b[1;32mIn [11], line 2\u001b[0m, in \u001b[0;36mdraw_colored_scatterplot\u001b[1;34m(data_df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_colored_scatterplot\u001b[39m(data_df):\n\u001b[1;32m----> 2\u001b[0m     sns\u001b[39m.\u001b[39;49mscatterplot(x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m'\u001b[39;49m, y\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m'\u001b[39;49m, hue\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcluster\u001b[39;49m\u001b[39m'\u001b[39;49m, style\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcluster\u001b[39;49m\u001b[39m'\u001b[39;49m, data\u001b[39m=\u001b[39;49mdata_df, legend\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\relational.py:742\u001b[0m, in \u001b[0;36mscatterplot\u001b[1;34m(data, x, y, hue, size, style, palette, hue_order, hue_norm, sizes, size_order, size_norm, markers, style_order, legend, ax, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscatterplot\u001b[39m(\n\u001b[0;32m    733\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m    734\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hue\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, style\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    739\u001b[0m ):\n\u001b[0;32m    741\u001b[0m     variables \u001b[39m=\u001b[39m _ScatterPlotter\u001b[39m.\u001b[39mget_semantics(\u001b[39mlocals\u001b[39m())\n\u001b[1;32m--> 742\u001b[0m     p \u001b[39m=\u001b[39m _ScatterPlotter(data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables, legend\u001b[39m=\u001b[39;49mlegend)\n\u001b[0;32m    744\u001b[0m     p\u001b[39m.\u001b[39mmap_hue(palette\u001b[39m=\u001b[39mpalette, order\u001b[39m=\u001b[39mhue_order, norm\u001b[39m=\u001b[39mhue_norm)\n\u001b[0;32m    745\u001b[0m     p\u001b[39m.\u001b[39mmap_size(sizes\u001b[39m=\u001b[39msizes, order\u001b[39m=\u001b[39msize_order, norm\u001b[39m=\u001b[39msize_norm)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\relational.py:538\u001b[0m, in \u001b[0;36m_ScatterPlotter.__init__\u001b[1;34m(self, data, variables, legend)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, variables\u001b[39m=\u001b[39m{}, legend\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m     \u001b[39m# TODO this is messy, we want the mapping to be agnostic about\u001b[39;00m\n\u001b[0;32m    532\u001b[0m     \u001b[39m# the kind of plot to draw, but for the time being we need to set\u001b[39;00m\n\u001b[0;32m    533\u001b[0m     \u001b[39m# this information so the SizeMapping can use it\u001b[39;00m\n\u001b[0;32m    534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_size_range \u001b[39m=\u001b[39m (\n\u001b[0;32m    535\u001b[0m         np\u001b[39m.\u001b[39mr_[\u001b[39m.5\u001b[39m, \u001b[39m2\u001b[39m] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msquare(mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m\"\u001b[39m\u001b[39mlines.markersize\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    536\u001b[0m     )\n\u001b[1;32m--> 538\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, variables\u001b[39m=\u001b[39;49mvariables)\n\u001b[0;32m    540\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegend \u001b[39m=\u001b[39m legend\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\_oldcore.py:640\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[39m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_ordered \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m}  \u001b[39m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 640\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_variables(data, variables)\n\u001b[0;32m    642\u001b[0m \u001b[39mfor\u001b[39;00m var, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_semantic_mappings\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    643\u001b[0m \n\u001b[0;32m    644\u001b[0m     \u001b[39m# Create the mapping function\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     map_func \u001b[39m=\u001b[39m partial(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmap, plotter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\_oldcore.py:701\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlong\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 701\u001b[0m     plot_data, variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_variables_longform(\n\u001b[0;32m    702\u001b[0m         data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mvariables,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_data \u001b[39m=\u001b[39m plot_data\n\u001b[0;32m    706\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariables \u001b[39m=\u001b[39m variables\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\seaborn\\_oldcore.py:938\u001b[0m, in \u001b[0;36mVectorPlotter._assign_variables_longform\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(val, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n\u001b[0;32m    934\u001b[0m \n\u001b[0;32m    935\u001b[0m     \u001b[39m# This looks like a column name but we don't know what it means!\u001b[39;00m\n\u001b[0;32m    937\u001b[0m     err \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not interpret value `\u001b[39m\u001b[39m{\u001b[39;00mval\u001b[39m}\u001b[39;00m\u001b[39m` for parameter `\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    940\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m     \u001b[39m# Otherwise, assume the value is itself data\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \n\u001b[0;32m    944\u001b[0m     \u001b[39m# Raise when data object is present and a vector can't matched\u001b[39;00m\n\u001b[0;32m    945\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pd\u001b[39m.\u001b[39mDataFrame) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(val, pd\u001b[39m.\u001b[39mSeries):\n",
            "\u001b[1;31mValueError\u001b[0m: Could not interpret value `X` for parameter `x`"
          ]
        }
      ],
      "source": [
        "print(target_cluster)\n",
        "draw_colored_scatterplot(target_cluster)\n",
        "#wordclouds(data_df, data, n_clusters)\n",
        "# tracks = get_target_cluster_songs(target_cluster, data_df, data)\n",
        "\n",
        "#generate_playlist(tracks)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
