{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "2s023VS6NvV5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import spotipy\n",
        "import sqlite3\n",
        "from sklearn.cluster import KMeans\n",
        "from sqlite3 import Error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "from openTSNE import TSNE as openTSNE\n",
        "from wordcloud import WordCloud\n",
        "from wordcloud import STOPWORDS as STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GWnx-v7ONvWA"
      },
      "outputs": [],
      "source": [
        "def connect_db(db_file):\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_file)\n",
        "    except Error as e:\n",
        "        print(e)\n",
        "    return conn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "IwvA8npiNvWB"
      },
      "outputs": [],
      "source": [
        "# connect to database\n",
        "# conn = connect_db('data.db')\n",
        "# cur = conn.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the target data\n",
        "# Extract playlists that have the input words in their titles to use as training data\n",
        "# input_words = ['summer', 'beach', 'throwbacks']\n",
        "# sql_select = '''SELECT AVG(danceability), AVG(energy), AVG(key), AVG(loudness), \n",
        "# AVG(mode), AVG(speechiness), AVG(acousticness), AVG(instrumentalness), AVG(liveness), \n",
        "# AVG(valence), AVG(tempo), AVG(duration_ms), AVG(time_signature) FROM avg_features_by_playlist WHERE'''\n",
        "def create_train(input_words, cur):\n",
        "    sql_select = '''SELECT * FROM avg_features_by_playlist WHERE'''\n",
        "    for w in range(len(input_words)):\n",
        "        sql_select += \" name LIKE '%\" + input_words[w] + \"%'\"\n",
        "        if w != len(input_words) -1:\n",
        "            sql_select += \" OR\" \n",
        "    pl_train = cur.execute(sql_select).fetchall()\n",
        "    return pl_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PCfDJLzmNvWB",
        "outputId": "994a0280-a025-4982-83fd-bfe5a120c0c3"
      },
      "outputs": [],
      "source": [
        "def get_x_data(pl_train, cur):\n",
        "    # Get X data (average playlist features)\n",
        "    data_cols = ['pid', 'name', 'danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature']\n",
        "    feature_cols = ['danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature']\n",
        "    test_cols = ['pid', 'name', 'danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','time_signature']\n",
        "\n",
        "    data_rows = cur.execute('select * from avg_features_by_playlist where pid > (select MIN(pid) from avg_features_by_playlist) order by pid').fetchall()\n",
        "\n",
        "\n",
        "    # test_pl = cur.execute('''select tracks_in_playlist.pid, tracks_in_playlist.track_uri, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature \n",
        "    # from features_by_track left join tracks_in_playlist on\n",
        "    # features_by_track.track_uri = tracks_in_playlist.track_uri \n",
        "    # where pid = (select MIN(pid) from avg_features_by_playlist)''').fetchall()\n",
        "\n",
        "\n",
        "    test_data = pd.DataFrame(pl_train, columns=test_cols)\n",
        "    data = pd.DataFrame(data_rows, columns=data_cols)\n",
        "\n",
        "    return test_data, test_cols, data, data_cols, feature_cols\n",
        "    # print(test_data.head())\n",
        "    # print(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ghXip9j4NvWD",
        "outputId": "b190b543-e694-46f0-a331-07272e7562a0"
      },
      "outputs": [],
      "source": [
        "# Scale the data\n",
        "\n",
        "def scale_data(test_data, data, feature_cols):\n",
        "    y = test_data[feature_cols].mean()\n",
        "\n",
        "    scaler = StandardScaler(with_mean=True, with_std=True).fit(data[feature_cols].values)\n",
        "\n",
        "    scaled_x = scaler.transform(data[feature_cols].values)\n",
        "    scaled_y = scaler.transform(np.array(y).reshape(1,-1))\n",
        "    scaled_features = pd.DataFrame(scaled_x)\n",
        "    tsne = openTSNE(perplexity=30, metric='euclidean', n_jobs=-1, random_state=0, verbose=False)\n",
        "    tsne_transformer = tsne.fit(scaled_x)\n",
        "    data_df = pd.DataFrame(tsne_transformer.transform(scaled_x), columns =['X', 'Y'])\n",
        "\n",
        "    #print(data_df, scaled_x, scaled_y, scaled_features)\n",
        "    return data_df, scaled_x, scaled_y, scaled_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qf9YiqzqNvWE",
        "outputId": "7cac83b7-e7e3-4125-a6ce-0f8ee47f6d64"
      },
      "outputs": [],
      "source": [
        "def draw_scatterplot(data_df):\n",
        "    sns.scatterplot(x='X', y='Y', data=data_df, legend=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "MxtQj6Khbe4e"
      },
      "outputs": [],
      "source": [
        "def calculate_num_clusters(scaled_x):\n",
        "    #TODO:\n",
        "    # needs to be tested and K updated below\n",
        "    #calculate how many K clusters there should be \n",
        "    wcss = [] \n",
        "    for number_of_clusters in range(1, 30): \n",
        "        kmeans = KMeans(n_clusters = number_of_clusters, random_state = 42)\n",
        "        kmeans.fit(scaled_x) \n",
        "        wcss.append(kmeans.inertia_)\n",
        "    wcss\n",
        "\n",
        "    ks = range(1, 30)\n",
        "    plt.plot(ks, wcss)\n",
        "    plt.axvline(4, linestyle='--', color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "HboT_UKqNvWF"
      },
      "outputs": [],
      "source": [
        "def kmeans_init(data_df, scaled_x, scaled_y, n_clusters):\n",
        "    # initialize KMeans\n",
        "    \n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    clusters = kmeans.fit(scaled_x)\n",
        "    labels = clusters.labels_\n",
        "    data_df['cluster'] = pd.Categorical(labels)\n",
        "\n",
        "    target_cluster = kmeans.predict(scaled_y)\n",
        "    print(target_cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "czsdHYnjNvWG",
        "outputId": "437142d0-73d2-4d1b-925b-0643ca9bddaa"
      },
      "outputs": [],
      "source": [
        "def draw_colored_scatterplot(data_df):\n",
        "    sns.scatterplot(x='X', y='Y', hue='cluster', style='cluster', data=data_df, legend=None)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wordclouds(data_df, data, n_clusters):\n",
        "    data_df[\"playlist_name\"] = data['name'].str.lower()\n",
        "    # display(data_df)\n",
        "\n",
        "    # we want to perform a pivot on data_df so that each cluster number is a column with row value equal to the playlist name.\n",
        "    # from there we can sum up that column to get the whole lsit of strings of playlist names for each cluster (column)\n",
        "    original_df = data_df.pivot(index='X', columns='cluster')['playlist_name'].reset_index()\n",
        "    original_df.columns.name = None\n",
        "    original_df = original_df.fillna('')\n",
        "    # original_df.head(20)\n",
        "    # print(original_df.columns)\n",
        "\n",
        "    #list of words to ignore\n",
        "    stop_words = STOPWORDS.update([\"i\", \"it\", \"me\", \"my\", \"that\", \"the\", \"of\", \"than\", \"then\", \n",
        "    \"when\", \"if\", \"a\", \"there\", \"playlist\", \"music\", \"song\", \"songs\", \"to\", \"too\", \"get\", \"as\", \"this\", \n",
        "    \"am\", \"is\", \"are\", \"has\", \"and\", \"aa\", \"aaa\", 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', \n",
        "    'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'])\n",
        "\n",
        "    wordclouds = {}\n",
        "    for i in range(n_clusters):\n",
        "        original_df.replace(np.nan, '')\n",
        "        original_df[i] = original_df[i].astype(str) #.sum(skipna=True))\n",
        "        wordclouds[\"wordcloud\" + str(i)] = WordCloud(stopwords=stop_words).generate(' '.join(original_df[i]))\n",
        "\n",
        "    for val in wordclouds.values():\n",
        "        plt.imshow(val)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_target_cluster_songs(target_cluster, data_df, data):\n",
        "    data_df[\"pid\"] = data['pid']\n",
        "    original_df = data_df.pivot(index='X', columns='cluster')['pid'].reset_index()\n",
        "    original_df.columns.name = None\n",
        "    target_cluster_df = original_df[target_cluster].dropna()\n",
        "    conn = connect_db('data.db')\n",
        "    cur = conn.cursor()\n",
        "    sql = '''SELECT features_by_track.* FROM features_by_track join tracks_in_playlist on features_by_track.track_uri = tracks_in_playlist.track_uri\n",
        "        WHERE '''\n",
        "    for pid in target_cluster_df[target_cluster[0]].values:\n",
        "        sql += \"tracks_in_playlist.pid = \" + str(int(pid))\n",
        "        sql += \" OR \"\n",
        "        sql = sql[:-3]\n",
        "        cols = [\"track_uri\",\n",
        "            \"danceability\",\n",
        "            \"energy\",\n",
        "            \"key\",\n",
        "            \"loudness\",\n",
        "            \"mode\",\n",
        "            \"speechiness\",\n",
        "            \"acousticness\",\n",
        "            \"instrumentalness\",\n",
        "            \"liveness\",\n",
        "            \"valence\",\n",
        "            \"tempo\",\n",
        "            \"duration_ms\",\n",
        "            \"time_signature\"]\n",
        "        \n",
        "    result = cur.execute(sql).fetchall()\n",
        "    \n",
        "    conn.close()\n",
        "    tracks = pd.DataFrame(result, columns = cols)\n",
        "    \n",
        "    print(tracks.head(5))\n",
        "    return tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "generate_playlist\n",
        "\n",
        "Parameters:\n",
        "input_words: (String) to pass to get_target_cluster_songs\n",
        "obscurity: (float) int\n",
        "max_song_length: (int) max_length of any song in the playlist\n",
        "use_minutes: (bool) use minutes instead of number of songs for playlist length (default false)\n",
        "playlist_length: number of songs in the playlist. If use_minutes=true, minutes in the playlist\n",
        "'''\n",
        "\n",
        "def generate_playlist(tracks, obscurity=1, max_song_length=10, use_minutes=False, playlist_length=10):\n",
        "    playlist = []\n",
        "    \n",
        "    \n",
        "    # obscurity\n",
        "    # length\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pipeline\n",
        "def pipeline():\n",
        "    # connect to database\n",
        "    conn = connect_db('data.db')\n",
        "    cur = conn.cursor()\n",
        "    input_words = [\"beach\",\"sun\"]\n",
        "    pl_train = create_train(input_words, cur)\n",
        "    test_data, test_cols, data, data_cols, feature_cols= get_x_data(pl_train, cur)\n",
        "    conn.close()\n",
        "    data_df, scaled_x, scaled_y, scaled_features = scale_data(test_data, data, feature_cols)\n",
        "    #draw_scatterplot(data_df) \n",
        "    #calculate_num_clusters(scaled_x) # unfinished\n",
        "    n_clusters = 20 # manually set after looking at calculate_num_clusters\n",
        "    target_cluster = kmeans_init(data_df, scaled_x, scaled_y, n_clusters)\n",
        "    #draw_colored_scatterplot(data_df)\n",
        "    #wordclouds(data_df, data, n_clusters)\n",
        "    tracks = get_target_cluster_songs(target_cluster, data_df, data)\n",
        "\n",
        "    generate_playlist(tracks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "None",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: None",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\alexp\\Documents\\GitHub\\cs254_playlist_project\\k_means_pipeline.ipynb Cell 16\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline()\n",
            "\u001b[1;32mc:\\Users\\alexp\\Documents\\GitHub\\cs254_playlist_project\\k_means_pipeline.ipynb Cell 16\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m target_cluster \u001b[39m=\u001b[39m kmeans_init(data_df, scaled_x, scaled_y, n_clusters)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#draw_colored_scatterplot(data_df)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#wordclouds(data_df, data, n_clusters)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m tracks \u001b[39m=\u001b[39m get_target_cluster_songs(target_cluster, data_df, data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m generate_playlist(tracks)\n",
            "\u001b[1;32mc:\\Users\\alexp\\Documents\\GitHub\\cs254_playlist_project\\k_means_pipeline.ipynb Cell 16\u001b[0m in \u001b[0;36mget_target_cluster_songs\u001b[1;34m(target_cluster, data_df, data)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m original_df \u001b[39m=\u001b[39m data_df\u001b[39m.\u001b[39mpivot(index\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m, columns\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mpid\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m original_df\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m target_cluster_df \u001b[39m=\u001b[39m original_df[target_cluster]\u001b[39m.\u001b[39mdropna()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m conn \u001b[39m=\u001b[39m connect_db(\u001b[39m'\u001b[39m\u001b[39mdata.db\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs254_playlist_project/k_means_pipeline.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n",
            "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: None"
          ]
        }
      ],
      "source": [
        "pipeline()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "771b2e09279bc50dd058e87d6b1c79418da63e1e1d4caf4728a325dde75439e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
